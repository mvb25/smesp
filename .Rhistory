error_cat  = c(1, 1)) %>%
run_simulation(sample_size = c(20, 20))
mydf<-specify_model(test="difference between means",
procedure = "null-hypothesis",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1))
input_data    = mydf
reps          = 1000
# original data
df <- original_data %>%
rename(x_obs    = attributes(.)$predictor_variable,
y_obs    = attributes(.)$response_variable)
# extract the error_cat values from the attributes of the original data
if(length(attributes(df)$error_cat) == 1){
mf <- c(attributes(df)$error_cat, attributes(df)$error_cat)
} else{
mf <- c(attributes(df)$error_cat)
}
# get the means of the levels of the categorical variable and the difference
# between these means
model_stats <- df %>%
group_by(x_obs) %>%
summarise(means = mean(y_obs)) %>%
ungroup() %>%
pivot_wider(names_from = x_obs, values_from = means) %>%
mutate(diff_means= .[[1,1]]-.[[1,2]])
# Get the overall mean plus the mean of the residuals (difference between
# group mean and observed values).
model_stats <- df %>%
group_by(x_obs) %>%
# the residuals are calculated as the difference between observed y and
# the group mean of the observed Y
mutate(residual  = y_obs - mean(y_obs)) %>%
# the following two lines mean that we calculate a single sd of the errors
# (observed - mean), i.e., we assume the same error term across groups. That
# might not be true (it is not true for the example data!). If you want to
# use the observed difference in error term, use 'heterosc_cat', based on
# values calculated prior to using this function.
ungroup() %>%
mutate(sd_resid = sd(residual)) %>%
summarise(mean_y_obs = mean(y_obs),
sd_resid = mean(sd_resid)) %>%
# multiply the error term with user-provided factor
bind_cols(model_stats, .)
# Define the size of the sample
if(length(sample_size) == 1){
if(sample_size == "as_data"){
nr_samples <- df %>% group_by(x_obs) %>% summarise(nr = n())
} else {
nr_samples <- data.frame(nr = c(sample_size, sample_size))
}
} else if(length(sample_size) == 2){
nr_samples <- data.frame(nr = sample_size)
}
# Get a random sample
if(attributes(df)$procedure == "confidence interval"){
# sample error term and add group means
new_sample <- data.frame(new_sample =
purrr::modify(rnorm(nr_samples$nr[1],
0, mf[1]*model_stats$sd_resid),
~ .x + model_stats[[1]]),
x_obs = rep(unique(df$x_obs)[1], nr_samples$nr[1])
)
new_sample <- data.frame(new_sample =
purrr::modify(rnorm(nr_samples$nr[2],
0, mf[2]*model_stats$sd_resid),
~ .x + model_stats[[2]]),
x_obs = rep(unique(df$x_obs)[2], nr_samples$nr[2])
) %>%
bind_rows(new_sample, .)
} else if(attributes(df)$procedure == "null-hypothesis"){
# sample error term and add mean_y_obs
new_sample <- data.frame(new_sample =
purrr::modify(rnorm(nr_samples$nr[1],
0, mf[1]*model_stats$sd_resid),
~ .x + model_stats$mean_y_obs),
x_obs = rep(unique(df$x_obs)[1], nr_samples$nr[1])
)
new_sample <- data.frame(new_sample =
purrr::modify(rnorm(nr_samples$nr[2],
0, mf[2]*model_stats$sd_resid),
~ .x + model_stats$mean_y_obs),
x_obs = rep(unique(df$x_obs)[2], nr_samples$nr[2])
) %>%
bind_rows(new_sample, .)
}
library(smesp)
mydf<-specify_model(test="difference between means",
procedure = "null-hypothesis",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
run_simulation(sample_size = c(20, 20))
specify_model(test="difference between means",
procedure = "null-hypothesis",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
run_simulation(sample_size = c(20, 20)) %>%
plot_distribution()
View(mydf)
new_sample <- new_sample %>% group_by(x_obs) %>% summarise(means = mean(new_sample))
View(new_sample)
x = rnorm(10)
y = rnorm(10)
t.test(x,y)
t.test(x,y)$estimate
new_sample <- data.frame(grp1 =
purrr::modify(rnorm(nr_samples$nr[1],
0, mf[1]*model_stats$sd_resid),
~ .x + model_stats$mean_y_obs)
)
new_sample <- data.frame(grp2 =
purrr::modify(rnorm(nr_samples$nr[2],
0, mf[2]*model_stats$sd_resid),
~ .x + model_stats$mean_y_obs)
) %>%
bind_cols(new_sample, .)
View(new_sample)
outcome <- t.test(rnorm(10),rnorm(10))
t.test(rnorm(10),rnorm(10))[1]
outcome <- t.test(rnorm(10),rnorm(10))[[1]]
outcome
outcome <- t.test(rnorm(10),rnorm(10))
t.test(rnorm(10),rnorm(10))[[1,3]]
t.test(rnorm(10),rnorm(10))[[1]][[3]]
t.test(rnorm(10),rnorm(10))[1,3]
t.test(rnorm(10),rnorm(10))[1:2]
t.test(rnorm(10),rnorm(10))[1,2]
t.test(new_sample)
t.test(new_sample$grp1, new_sample$grp2)
outcome <- t.test(new_sample$grp1, new_sample$grp2)
tmp1 <- t.test(new_sample$grp1, new_sample$grp2)
tmp1[[1]]
tmp1[[5]]
tmp1[[5]][1]
tmp1[[5]][1]-tmp1[[5]][2]
reps=1000
outcome <- data.frame(t-value = numeric(reps), p-value=numeric(reps), diff-means = numeric(reps))
outcome <- data.frame(t_value = numeric(reps), p_value = numeric(reps), diff_means = numeric(reps))
View(outcome)
for(i in 1:reps){
# Get a random sample
if(attributes(df)$procedure == "confidence interval"){
# sample error term and add group means
new_sample <- data.frame(new_sample =
purrr::modify(rnorm(nr_samples$nr[1],
0, mf[1]*model_stats$sd_resid),
~ .x + model_stats[[1]]),
x_obs = rep(unique(df$x_obs)[1], nr_samples$nr[1])
)
new_sample <- data.frame(new_sample =
purrr::modify(rnorm(nr_samples$nr[2],
0, mf[2]*model_stats$sd_resid),
~ .x + model_stats[[2]]),
x_obs = rep(unique(df$x_obs)[2], nr_samples$nr[2])
) %>%
bind_rows(new_sample, .)
} else if(attributes(df)$procedure == "null-hypothesis"){
# sample error term and add mean_y_obs
new_sample <- data.frame(grp1 =
purrr::modify(rnorm(nr_samples$nr[1],
0, mf[1]*model_stats$sd_resid),
~ .x + model_stats$mean_y_obs)
)
new_sample <- data.frame(grp2 =
purrr::modify(rnorm(nr_samples$nr[2],
0, mf[2]*model_stats$sd_resid),
~ .x + model_stats$mean_y_obs)
) %>%
bind_cols(new_sample, .)
}
tmp1 <- t.test(new_sample$grp1, new_sample$grp2, var.equal=TRUE)
outcome[i,1] <- tmp1[[1]]
outcome[i,2] <- tmp1[[3]]
outcome[i,3] <- tmp1[[5]][1]-tmp1[[5]][2]
}
# Get a random sample
if(attributes(df)$procedure == "confidence interval"){
# sample error term and add group means
new_sample <- data.frame(new_sample =
purrr::modify(rnorm(nr_samples$nr[1],
0, mf[1]*model_stats$sd_resid),
~ .x + model_stats[[1]]),
x_obs = rep(unique(df$x_obs)[1], nr_samples$nr[1])
)
new_sample <- data.frame(new_sample =
purrr::modify(rnorm(nr_samples$nr[2],
0, mf[2]*model_stats$sd_resid),
~ .x + model_stats[[2]]),
x_obs = rep(unique(df$x_obs)[2], nr_samples$nr[2])
) %>%
bind_rows(new_sample, .)
} else if(attributes(df)$procedure == "null-hypothesis"){
# sample error term and add mean_y_obs
new_sample <- data.frame(grp1 =
purrr::modify(rnorm(nr_samples$nr[1],
0, mf[1]*model_stats$sd_resid),
~ .x + model_stats$mean_y_obs)
)
new_sample <- data.frame(grp2 =
purrr::modify(rnorm(nr_samples$nr[2],
0, mf[2]*model_stats$sd_resid),
~ .x + model_stats$mean_y_obs)
) %>%
bind_cols(new_sample, .)
}
tmp1 <- t.test(new_sample$grp1, new_sample$grp2, var.equal=TRUE)
# sample error term and add group means
new_sample <- data.frame(new_sample =
purrr::modify(rnorm(nr_samples$nr[1],
0, mf[1]*model_stats$sd_resid),
~ .x + model_stats[[1]]),
x_obs = rep(unique(df$x_obs)[1], nr_samples$nr[1])
)
new_sample <- data.frame(new_sample =
purrr::modify(rnorm(nr_samples$nr[2],
0, mf[2]*model_stats$sd_resid),
~ .x + model_stats[[2]]),
x_obs = rep(unique(df$x_obs)[2], nr_samples$nr[2])
) %>%
bind_rows(new_sample, .)
tmp1 <- t.test(new_sample$grp1, new_sample$grp2, var.equal=TRUE)
# sample error term and add group means
new_sample <- data.frame(new_sample =
purrr::modify(rnorm(nr_samples$nr[1],
0, mf[1]*model_stats$sd_resid),
~ .x + model_stats[[1]]),
x_obs = rep(unique(df$x_obs)[1], nr_samples$nr[1])
)
new_sample <- data.frame(new_sample =
purrr::modify(rnorm(nr_samples$nr[2],
0, mf[2]*model_stats$sd_resid),
~ .x + model_stats[[2]]),
x_obs = rep(unique(df$x_obs)[2], nr_samples$nr[2])
) %>%
bind_cols(new_sample, .)
rm(new_sample)
# sample error term and add group means
new_sample <- data.frame(new_sample =
purrr::modify(rnorm(nr_samples$nr[1],
0, mf[1]*model_stats$sd_resid),
~ .x + model_stats[[1]])
)
tmp1 <- t.test(new_sample$grp1, new_sample$grp2, var.equal=TRUE)
# sample error term and add group means
new_sample <- data.frame(grp1 =
purrr::modify(rnorm(nr_samples$nr[1],
0, mf[1]*model_stats$sd_resid),
~ .x + model_stats[[1]])
)
new_sample <- data.frame(grp2 =
purrr::modify(rnorm(nr_samples$nr[2],
0, mf[2]*model_stats$sd_resid),
~ .x + model_stats[[2]])
) %>%
bind_cols(new_sample, .)
tmp1 <- t.test(new_sample$grp1, new_sample$grp2, var.equal=TRUE)
outcome[i,1] <- tmp1[[1]]
outcome[i,2] <- tmp1[[3]]
outcome[i,3] <- tmp1[[5]][1]-tmp1[[5]][2]
View(outcome)
library(smesp)
library(smesp)
library(smesp)
specify_model(test="difference between means",
procedure = "null-hypothesis",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
run_simulation(sample_size = c(20, 20)) %>%
plot_distribution()
mydf<-specify_model(test="difference between means",
procedure = "null-hypothesis",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
run_simulation(sample_size = c(20, 20))
library(smesp)
specify_model(test="difference between means",
procedure = "null-hypothesis",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
run_simulation(sample_size = c(20, 20)) %>%
plot_distribution()
specify_model(test="difference between means",
procedure = "null-hypothesis",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
run_simulation(reps = 5000, sample_size = c(200, 200)) %>%
plot_distribution()
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
run_simulation(reps = 5000, sample_size = c(200, 200)) %>%
plot_distribution()
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
run_simulation(reps = 5000, sample_size = c(10, 10)) %>%
plot_distribution()
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
run_simulation(reps = 1000, sample_size = c(2, 2)) %>%
plot_distribution()
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
run_simulation(reps = 5000, sample_size = c(2, 2)) %>%
plot_distribution()
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
run_simulation(reps = 1000, sample_size = c(10, 100)) %>%
plot_distribution()
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
run_simulation(reps = 1000, sample_size = c(10, 20)) %>%
plot_distribution()
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
run_simulation(reps = 1000, sample_size = c(10, 20))
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
visualise_simulation(reps = 1000, sample_size = c(10, 20))
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
visualise_simulation(sample_size = c(10, 20))
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
visualise_simulation(sample_size = c(10, 100))
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
visualise_simulation(reps = 1000, sample_size = c(10, 100))
sample_size = c(10, 20)
# original data
df <- input_data %>%
rename(x_obs    = attributes(.)$predictor_variable,
y_obs    = attributes(.)$response_variable)
# extract the error_cat values from the attributes of the original data
if(length(attributes(df)$error_cat) == 1){
mf <- c(attributes(df)$error_cat, attributes(df)$error_cat)
} else{
mf <- c(attributes(df)$error_cat)
}
# get the means of the levels of the categorical variable and the difference
# between these means
model_stats <- df %>%
group_by(x_obs) %>%
summarise(means = mean(y_obs)) %>%
ungroup() %>%
pivot_wider(names_from = x_obs, values_from = means) %>%
mutate(diff_means= .[[1,1]]-.[[1,2]])
# Get the overall mean plus the mean of the residuals (difference between
# group mean and observed values).
model_stats <- df %>%
group_by(x_obs) %>%
# the residuals are calculated as the difference between observed y and
# the group mean of the observed Y
mutate(residual  = y_obs - mean(y_obs)) %>%
# the following two lines mean that we calculate a single sd of the errors
# (observed - mean), i.e., we assume the same error term across groups. That
# might not be true (it is not true for the example data!). If you want to
# use the observed difference in error term, use 'heterosc_cat', based on
# values calculated prior to using this function.
ungroup() %>%
mutate(sd_resid = sd(residual)) %>%
summarise(mean_y_obs = mean(y_obs),
sd_resid = mean(sd_resid)) %>%
# multiply the error term with user-provided factor
bind_cols(model_stats, .)
# Define the size of the sample
if(length(sample_size) == 1){
if(sample_size == "as_data"){
nr_samples <- df %>% group_by(x_obs) %>% summarise(nr = n())
} else {
nr_samples <- data.frame(nr = c(sample_size, sample_size))
}
} else if(length(sample_size) == 2){
nr_samples <- data.frame(nr = sample_size)
}
library(smesp)
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 1)) %>%
run_simulation(reps = 1000, sample_size = c(10, 100)) %>%
plot_distribution()
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 5)) %>%
run_simulation(reps = 1000, sample_size = c(10, 100)) %>%
plot_distribution()
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 50)) %>%
run_simulation(reps = 1000, sample_size = c(10, 100)) %>%
plot_distribution()
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(1, 50)) %>%
run_simulation(reps = 5000, sample_size = c(10, 100)) %>%
plot_distribution()
specify_model(test="difference between means",
procedure = "confidence interval",
cat_var = "top_pos",
cont_var_1  = NULL,
error_cat  = c(0.2, 4)) %>%
run_simulation(reps = 5000, sample_size = c(10, 100)) %>%
plot_distribution()
pollen <- read.csv("phyloflash_plants-pollen_data.csv")
pollen <- read.csv("data/phyloflash_plants-pollen_data.csv")
View(pollen)
library(tidyverse)
pollen <- read.csv("data/phyloflash_plants-pollen_data.csv") %>%
pivot_longer(names_to = sample_id, values_to = abundance)
pollen <- read.csv("data/phyloflash_plants-pollen_data.csv") %>%
pivot_longer(cols = 4:64, names_to = sample_id, values_to = abundance)
pollen <- read.csv("data/phyloflash_plants-pollen_data.csv") %>%
pivot_longer(cols = 4:64, names_to = "sample_id", values_to = "abundance")
View(pollen)
pollen <- read.csv("data/phyloflash_plants-pollen_data.csv") %>%
pivot_longer(cols = 4:64, names_to = "sample_id", values_to = "abundance") %>%
filter(abundance != 0)
pollen <- read.csv("data/phyloflash_plants-pollen_additional.csv") %>%
left_join(pollen, ., by = c("sample_id", "sample_id"))
View(pollen)
pollen <- read.csv("data/phyloflash_plants-pollen_additional.csv") %>%
left_join(pollen, .)
View(pollen)
pollen <- read.csv("data/phyloflash_plants-pollen_data.csv") %>%
pivot_longer(cols = 4:64, names_to = "sample_id", values_to = "abundance") %>%
filter(abundance != 0)
pollen <- read.csv("data/phyloflash_plants-pollen_additional.csv") %>%
left_join(pollen)
write.csv(pollen, "data/pollen_data.csv", row.names = F)
View(pollen)
library(smesp)
specify_model() %>% visualise_simulation()
specify_model() %>% visualise_simulation()
specify_model() %>% visualise_simulation()
specify_model() %>% visualise_simulation()
specify_model() %>% visualise_simulation()
specify_model() %>% visualise_simulation()
specify_model() %>% visualise_simulation()
specify_model() %>% visualise_simulation()
specify_model() %>% visualise_simulation()
specify_model() %>% visualise_simulation()
specify_model() %>% visualise_simulation()
library(devtools)
install_github("mvb/smesp")
install_github("mvb25/smesp")
specify_model() %>% visualise_simulation()
specify_model() %>% visualise_simulation()
install_github("mvb25/smesp")
install.packages(c("ade4", "backports", "bayestestR", "BiodiversityR", "blogdown", "broom", "cli", "colorspace", "cowplot", "cpp11", "data.table", "dbplyr", "digest", "DT", "entropart", "fda", "fitdistrplus", "future", "generics", "GGally", "ggeffects", "ggplot2", "ggrepel", "ggridges", "gh", "git2r", "globals", "here", "Hmisc", "htmlwidgets", "insight", "isoband", "janitor", "jsonlite", "lme4", "lubridate", "magrittr", "moderndive", "multcomp", "openxlsx", "patchwork", "pbkrtest", "performance", "pillar", "processx", "quantreg", "R6", "raster", "RcppArmadillo", "RcppEigen", "renv", "rgl", "rlang", "rmarkdown", "rsq", "servr", "sp", "testthat", "tinytex", "treemapify", "usethis", "vctrs", "vegan", "VGAM", "xfun"))
remove.packages(smesp)
remove.packages("smesp")
specify_model()
install_github("mvb25/smesp")
library(devtools)
install_github("mvb25/smesp")
specify_model() %>% visualise_simulation()
library(tidyverse)
specify_model() %>% visualise_simulation()
library(smesp)
specify_model() %>% visualise_simulation()
help(package = smesp)
?smesp
help(package = smesp)
?summarise
?specify_model
packageDescription("smesp")
